{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9f12ae1c-144f-4a9b-b530-99ac93889eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# These are standard python modules\n",
    "import json, time, urllib.parse\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab0799b-5184-4adc-bf11-5345a95a5a7e",
   "metadata": {},
   "source": [
    "The CSV file is imported. The list of articles is extracted from the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "63befae3-652a-4c05-bd1a-7112be0f1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the CSV file\n",
    "df = pandas.read_csv('us_cities_by_state_SEPT.2023.csv')\n",
    "article_name_list = list(set(df['page_title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68402f55-9863-4992-8c4c-5da1a69f20ff",
   "metadata": {},
   "source": [
    "A subset of the above dataframe containing just the article title and the state is saved to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "f38ab541-5aac-4c13-a675-42f98d6b099b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_title</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22152</th>\n",
       "      <td>Wamsutter, Wyoming</td>\n",
       "      <td>Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22153</th>\n",
       "      <td>Wheatland, Wyoming</td>\n",
       "      <td>Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22154</th>\n",
       "      <td>Worland, Wyoming</td>\n",
       "      <td>Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22155</th>\n",
       "      <td>Wright, Wyoming</td>\n",
       "      <td>Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22156</th>\n",
       "      <td>Yoder, Wyoming</td>\n",
       "      <td>Wyoming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22157 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                page_title    state\n",
       "0       Abbeville, Alabama  Alabama\n",
       "1      Adamsville, Alabama  Alabama\n",
       "2         Addison, Alabama  Alabama\n",
       "3           Akron, Alabama  Alabama\n",
       "4       Alabaster, Alabama  Alabama\n",
       "...                    ...      ...\n",
       "22152   Wamsutter, Wyoming  Wyoming\n",
       "22153   Wheatland, Wyoming  Wyoming\n",
       "22154     Worland, Wyoming  Wyoming\n",
       "22155      Wright, Wyoming  Wyoming\n",
       "22156       Yoder, Wyoming  Wyoming\n",
       "\n",
       "[22157 rows x 2 columns]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_article_mapping = df[[\"page_title\", \"state\"]]\n",
    "state_article_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663a3dd-a804-4cee-9c65-e9cf02a84eaf",
   "metadata": {},
   "source": [
    "A few constants are defined that make it easier to access certain values to enhance readability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b49056ce-8cd6-4f03-b962-ec2138d80007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<anair4@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = article_name_list \n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e95c36f9-8183-4003-af40-9bdc7ab75c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "#    The current LiftWing ORES API endpoint and prediction model\n",
    "#\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "#\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "#\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (60.0/5000.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "#    When making automated requests we should include something that is unique to the person making the request\n",
    "#    This should include an email - your UW email would be good to put in there\n",
    "#    \n",
    "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "#    as part of the header too\n",
    "#\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"anair4@uw.edu, University of Washington, MSDS DATA 512 - AUTUMN 2023\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "#\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "#\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"\",         # your email address should go here\n",
    "    'access_token'  : \"\"          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "#\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "#\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n",
    "#\n",
    "#    These are used later - defined here so they, at least, have empty values\n",
    "#\n",
    "USERNAME = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe95725-8290-4031-bd0b-6ca9dee33ed4",
   "metadata": {},
   "source": [
    "The request_pageinfo_per_article is a function that creates and sends a request by combining the endpoint url with the parameters for the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1d8f981-22a1-472e-840c-1eca23f5b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c734b37-b050-4903-a3e1-027b15b6fce3",
   "metadata": {},
   "source": [
    "The function created above is run for each of the articles which are stored in a list which is iterated through. The last revision ID is extracted from the response and compiled into a dictionary which is stored in a JSON file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb05ad9-b531-40a9-bc3e-5acf4891286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_id_info = dict()\n",
    "for i in range(0, len(ARTICLE_TITLES)):\n",
    "    print(f\"Getting page info data for: {ARTICLE_TITLES[i]}\")\n",
    "    info = request_pageinfo_per_article(ARTICLE_TITLES[i])\n",
    "    temp_key = list(info['query']['pages'].keys())[0]\n",
    "    rev_id_info[ARTICLE_TITLES[i]] = info['query']['pages'][temp_key]['lastrevid']\n",
    "    \n",
    "with open(\"rev-id-details.json\", \"w\") as final:\n",
    "   json.dump(rev_id_info, final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b99cfb1-4174-4cf0-a6c3-0b193d98a4fa",
   "metadata": {},
   "source": [
    "The username and access code were generated from the Wikimedia page. The link is https://api.wikimedia.org/wiki/Authentication#:~:text=Create%20token,place%2C%20like%20a%20password%20manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d537fdd7-4841-4e02-86c6-3a70f8158264",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = \"Anair12\"\n",
    "ACCESS_TOKEN = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiJmNTAyNmQ3MjhmNWFlZDk0YTFhYWJjN2VmMDQ2MmI3MSIsImp0aSI6ImU5NDA3NDQzNzI5NDFkYjQ2N2UxYjE0NjA2NmM2N2NlMTRhNDZhYjhlYTEzNzk4M2E5ZmQ2MzA2MTg0MGVkMmRjMThhYmE1YmUyNzg5MmY4IiwiaWF0IjoxNjk3MzI0ODk1LjE1NDA1LCJuYmYiOjE2OTczMjQ4OTUuMTU0MDUzLCJleHAiOjMzMjU0MjMzNjk1LjE1Mjc1Niwic3ViIjoiNzQwMDU5NTgiLCJpc3MiOiJodHRwczovL21ldGEud2lraW1lZGlhLm9yZyIsInJhdGVsaW1pdCI6eyJyZXF1ZXN0c19wZXJfdW5pdCI6NTAwMCwidW5pdCI6IkhPVVIifSwic2NvcGVzIjpbImJhc2ljIl19.tMHApBeuauq1kVBM1OW2_aQwh_ANrMhRXut2FlaKFc-XrurT_U-QrJkjECewRCU54MiEwKCwxZWX4QEWXjHgMwRlRAm1Qj11yA5As8Br6f_QnWit4B4XBv_MfBCmTVphhQLhfXN6rnWmbaPQJR1M4VRg0y3NXhaZ8ExSLuJUk6BsiZcFM-vNLZHtyTr0op8FkQd5w6XdgozXoAq76ggMFHnj4_yJ3rtKy3cuPlnSUQpHiTYXLorHcN--9b_RV5SPRGFxrZ1D9oNjajzg9p0ENLnSB9UmES-3ZBSAhDPUcpRzUfuyVpyzMlyUZIG_UIfzI2sRXPGEpoTNYiOzKOrhV_FdwosAVjIgfqd5eCGjPbYo7W6eGebYAHDTcqibFGSRWR3LqG21RS01a0p77vLU-zklNuqgSDfLfof3Mr15yckS8TMY4Mw9-T3g5SBbOFSFOkiSaAT-sElfp2nVkfmDgrj_izv_AFweIEpXwkxnwRU8IvnmpqPujZoIAQ-eJFOnl7lNGUuLy9OlBX5XuJiAoDiCKYsBmzsuOP7ac69MpS4THjE3ZnPbOnXAVulTe2WGDU0LH8VM3kUPnGo7n3S8RXlu5Ub5Yo6E-DRDvOASL5w7PDLbog6qOsvI0jwLCRDysLMdfG3-rfOvD0CqBNzzOVnOheGiuiu2lWONIjESH2g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4f3e9-c817-40b9-b31c-0ff25f16f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Decode the Wikimedia JWT Access token\n",
    "#\n",
    "#   NOTE: This is not required to use LiftWing to request ORES scores. This is just being done to satisfy my curiosity.\n",
    "#\n",
    "import base64\n",
    "\n",
    "print(\"Decoding the ACCESS_TOKEN:\")\n",
    "try:\n",
    "    token_components = ACCESS_TOKEN.split(\".\")\n",
    "    if len(token_components) == 3:\n",
    "        header = json.loads(base64.b64decode(token_components[0]).decode())\n",
    "        payload = json.loads(base64.b64decode(token_components[1]).decode())\n",
    "        print(\"Token Header:\",json.dumps(header,indent=4))\n",
    "        print(\"Token Payload:\",json.dumps(payload,indent=4))\n",
    "        #print(\"Token Signature:\",token_components[2])\n",
    "        print(\"Token Signature: <value_suppressed>\")\n",
    "        #\n",
    "        #  One should be able to use public/private keys to actually validate that signature - left as an exercise for later\n",
    "        #\n",
    "    else:\n",
    "        print(f\"The ACCESS_TOKEN appears to be improperly structured. It should have 3 components and it has {len(token_components)}\")\n",
    "except Exception as ex:\n",
    "    print(f\"Looks like the ACCESS_TOKEN is undefined or an empty value\")\n",
    "    raise(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa0cdd-2268-4f1c-b1e3-c44082811ddb",
   "metadata": {},
   "source": [
    "The revision IDs extracted are used as parameters along with the username and access token in order to create and send a request using the ORES API which allows use to retrieve information about the predicted quality scores of each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8f9dfcd-4965-46b8-9f9a-82fd3bab1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056998f4-d91c-428b-b9c6-1c7bce3a7909",
   "metadata": {},
   "source": [
    "The above function is called on all of the articles and the predicted quality scores are stored in a dictionary which is written into a JSON file. Ensure that the the limit is not exceeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c45ef02-122a-460f-94a2-94043fadaf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rev-id-details.json\", \"r\") as final:\n",
    "   rev_id_scores = json.load(final)\n",
    "rev_id_scores_final = {}\n",
    "bad_recs = {}\n",
    "for j in range(0, len(ARTICLE_TITLES)):\n",
    "    rev_id = 0\n",
    "    article_title = ARTICLE_TITLES[j]\n",
    "    print(article_title, j)\n",
    "    #print(f\"Getting LiftWing ORES scores for '{article_title}' with revid: {rev_id_scores[article_title]:d}\")\n",
    "#\n",
    "#    Make the call, just pass in the article revision ID, email address, and access token\n",
    "    score = request_ores_score_per_article(article_revid=rev_id_scores[article_title],\n",
    "                                       email_address=\"anair4@uw.edu\",\n",
    "                                       access_token=ACCESS_TOKEN)\n",
    "    if score != None:\n",
    "        rev_id = int(rev_id_scores[article_title])\n",
    "        rev_id_scores_final[article_title] = {}\n",
    "        rev_id_scores_final[article_title][\"revision_id\"] = rev_id\n",
    "        rev_id_scores_final[article_title][\"score\"] = score[\"enwiki\"][\"scores\"]\n",
    "    else:\n",
    "        rev_id = int(rev_id_scores[article_title])\n",
    "        bad_recs[article_title][\"rev_id\"] = rev_id\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072b817a-cb1d-455f-a2d7-68b65c64e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dump the results into a JSON file to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2cc19e16-e3bc-4d4a-b4eb-2dd53c1e9122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Output the result\n",
    "with open(\"score-details.json\", \"w\") as final:\n",
    "   json.dump(rev_id_scores_final, final)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7085cb42-8229-4b21-bef4-a183e613375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rev-id-details.json\", \"r\") as final:\n",
    "   rev_id_scores_final2 = json.load(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643c708-1f8b-4812-bb07-03c3c208444c",
   "metadata": {},
   "source": [
    "Since the number of requests was so high, the API would stop responding after every 3000-4000 requests with a 429 error code signifying too many requests. So, the dictionary was saved into a file in order to save the responses that were given and then the function was run again to update the doctionary with the remaining values. The process takes about 7 hours in total. A dictionary to store the unclassified articles was created but all the articles were given a classification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6210e0e1-01b9-49ed-b980-f101f2ba5109",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_recs2 = {}\n",
    "for j in range(8925, len(ARTICLE_TITLES)):\n",
    "    article_title = ARTICLE_TITLES[j]\n",
    "    print(article_title, j)\n",
    "    #print(f\"Getting LiftWing ORES scores for '{article_title}' with revid: {rev_id_scores[article_title]:d}\")\n",
    "#\n",
    "#    Make the call, just pass in the article revision ID, email address, and access token\n",
    "    score = request_ores_score_per_article(article_revid=rev_id_scores[article_title],\n",
    "                                       email_address=\"anair4@uw.edu\",\n",
    "                                       access_token=ACCESS_TOKEN)\n",
    "    if score != None:\n",
    "        rev_id = int(rev_id_scores[article_title])\n",
    "        rev_id_scores_final[article_title] = {}\n",
    "        rev_id_scores_final[article_title][\"revision_id\"] = rev_id\n",
    "        rev_id_scores_final[article_title][\"score\"] = score[\"enwiki\"][\"scores\"]\n",
    "    else:\n",
    "        rev_id = int(rev_id_scores[article_title])\n",
    "        bad_recs[article_title][\"rev_id\"] = rev_id\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "158b59a2-3668-4e27-93f7-e4f9e4be53ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Output the result\n",
    "with open(\"score-details2.json\", \"w\") as final:\n",
    "   json.dump(rev_id_scores_final, final)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9197840-b88e-49ff-9c7f-ca52bcd64080",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_recs3 = {}\n",
    "for j in range(11778, len(ARTICLE_TITLES)):\n",
    "    article_title = ARTICLE_TITLES[j]\n",
    "    print(article_title, j)\n",
    "    #print(f\"Getting LiftWing ORES scores for '{article_title}' with revid: {rev_id_scores[article_title]:d}\")\n",
    "#\n",
    "#    Make the call, just pass in the article revision ID, email address, and access token\n",
    "    score = request_ores_score_per_article(article_revid=rev_id_scores[article_title],\n",
    "                                       email_address=\"anair4@uw.edu\",\n",
    "                                       access_token=ACCESS_TOKEN)\n",
    "    if score != None:\n",
    "        rev_id = int(rev_id_scores[article_title])\n",
    "        rev_id_scores_final[article_title] = {}\n",
    "        rev_id_scores_final[article_title][\"revision_id\"] = rev_id\n",
    "        rev_id_scores_final[article_title][\"score\"] = score[\"enwiki\"][\"scores\"]\n",
    "    else:\n",
    "        rev_id = int(rev_id_scores[article_title])\n",
    "        bad_recs[article_title][\"rev_id\"] = rev_id\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ae507b1b-864b-42ec-a5cb-97e8aca18446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Output the result\n",
    "with open(\"score-details3.json\", \"w\") as final:\n",
    "   json.dump(rev_id_scores_final, final)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5aec02-6606-4368-8683-c0bf1599e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_recs4 = {}\n",
    "for j in range(14533, len(ARTICLE_TITLES)):\n",
    "    article_title = ARTICLE_TITLES[j]\n",
    "    print(article_title, j)\n",
    "    #print(f\"Getting LiftWing ORES scores for '{article_title}' with revid: {rev_id_scores[article_title]:d}\")\n",
    "#\n",
    "#    Make the call, just pass in the article revision ID, email address, and access token\n",
    "    score = request_ores_score_per_article(article_revid=rev_id_scores[article_title],\n",
    "                                       email_address=\"anair4@uw.edu\",\n",
    "                                       access_token=ACCESS_TOKEN)\n",
    "    if score != None:\n",
    "        rev_id = int(rev_id_scores[article_title])\n",
    "        rev_id_scores_final[article_title] = {}\n",
    "        rev_id_scores_final[article_title][\"revision_id\"] = rev_id\n",
    "        rev_id_scores_final[article_title][\"score\"] = score[\"enwiki\"][\"scores\"]\n",
    "    else:\n",
    "        rev_id = int(rev_id_scores[article_title])\n",
    "        bad_rec4[article_title][\"rev_id\"] = rev_id\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "21ce98e0-c33c-451f-9a22-e27590bdb939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Output the result\n",
    "with open(\"score-details4.json\", \"w\") as final:\n",
    "   json.dump(rev_id_scores_final, final)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896b535-ffe3-4763-bd71-d21c3542c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_recs5 = {}\n",
    "for j in range(21026, len(ARTICLE_TITLES)):\n",
    "    article_title = ARTICLE_TITLES[j]\n",
    "    print(article_title, j)\n",
    "    #print(f\"Getting LiftWing ORES scores for '{article_title}' with revid: {rev_id_scores[article_title]:d}\")\n",
    "#\n",
    "#    Make the call, just pass in the article revision ID, email address, and access token\n",
    "    score = request_ores_score_per_article(article_revid=rev_id_scores[article_title],\n",
    "                                       email_address=\"anair4@uw.edu\",\n",
    "                                       access_token=ACCESS_TOKEN)\n",
    "    if score != None:\n",
    "        rev_id = int(rev_id_scores[article_title])\n",
    "        rev_id_scores_final[article_title] = {}\n",
    "        rev_id_scores_final[article_title][\"revision_id\"] = rev_id\n",
    "        rev_id_scores_final[article_title][\"score\"] = score[\"enwiki\"][\"scores\"]\n",
    "    else:\n",
    "        rev_id = int(rev_id_scores[article_title])\n",
    "        bad_rec5[article_title][\"rev_id\"] = rev_id\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "38546b38-0b5f-4755-9654-1529b2a87f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Output the result\n",
    "with open(\"score-details5.json\", \"w\") as final:\n",
    "   json.dump(rev_id_scores_final, final)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0f278-453c-4ca6-b382-39a791eb1e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_recs5 = {}\n",
    "for j in range(21144, len(ARTICLE_TITLES)):\n",
    "    article_title = ARTICLE_TITLES[j]\n",
    "    print(article_title, j)\n",
    "    #print(f\"Getting LiftWing ORES scores for '{article_title}' with revid: {rev_id_scores[article_title]:d}\")\n",
    "#\n",
    "#    Make the call, just pass in the article revision ID, email address, and access token\n",
    "    score = request_ores_score_per_article(article_revid=rev_id_scores[article_title],\n",
    "                                       email_address=\"anair4@uw.edu\",\n",
    "                                       access_token=ACCESS_TOKEN)\n",
    "    if score != None:\n",
    "        rev_id = int(rev_id_scores[article_title])\n",
    "        rev_id_scores_final[article_title] = {}\n",
    "        rev_id_scores_final[article_title][\"revision_id\"] = rev_id\n",
    "        rev_id_scores_final[article_title][\"score\"] = score[\"enwiki\"][\"scores\"]\n",
    "    else:\n",
    "        rev_id = int(rev_id_scores[article_title])\n",
    "        bad_rec5[article_title][\"rev_id\"] = rev_id\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "698dcbf0-6d74-4e5b-b09b-ac2037fe48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Output the result\n",
    "with open(\"score-details6.json\", \"w\") as final:\n",
    "   json.dump(rev_id_scores_final, final)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "5b01b7b1-eea0-4828-baae-cbfdb5a65e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_title_list = []\n",
    "state_list = []\n",
    "rev_id_list = []\n",
    "classification_list = []\n",
    "for key in list(rev_id_scores_final.keys()):\n",
    "    split = key.split(',')\n",
    "    l = len(split)\n",
    "    state = df.loc[df['page_title'] == key, 'state']\n",
    "    rev_id = rev_id_scores_final[key][\"revision_id\"]\n",
    "    state_list.append(state.values[0].replace('_', ' '))\n",
    "    rev_id_list.append(rev_id)\n",
    "    article_title_list.append(key)\n",
    "    classification_list.append(rev_id_scores_final[key][\"score\"][str(rev_id)][\"articlequality\"][\"score\"][\"prediction\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84960d2-439e-49a6-aa13-6c98275468c0",
   "metadata": {},
   "source": [
    "A new dataframe is created by combining the article title, revision id, state and the article quality which is the classification of the article from the ORES API. This information is taken from the dictionary that was created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "6bfb7781-ca89-4c09-8009-38399f9481c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"state\":state_list, \"article_title\":article_title_list, \"revision_id\":rev_id_list, \"article_quality\":classification_list}\n",
    "data_to_analyze = pandas.DataFrame(data)\n",
    "data_to_analyze.to_csv('wp_scored_city_articles_by_state.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405fe6f-f233-42ef-9be6-0ac1dc773b79",
   "metadata": {},
   "source": [
    "The US States by Region - US Census Bureau sheet is read in which specifies the regional division that each state belongs to. Using the ffill() function, the file is formatted in order to create a mapping between regional division and state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "d6b334be-a241-417f-a76b-37e09c17e123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New England</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New England</td>\n",
       "      <td>Connecticut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New England</td>\n",
       "      <td>Maine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New England</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>Alaska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>Hawaii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>Oregon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   regional_division          state\n",
       "0                NaN            NaN\n",
       "1        New England            NaN\n",
       "2        New England    Connecticut\n",
       "3        New England          Maine\n",
       "4        New England  Massachusetts\n",
       "..               ...            ...\n",
       "58           Pacific         Alaska\n",
       "59           Pacific     California\n",
       "60           Pacific         Hawaii\n",
       "61           Pacific         Oregon\n",
       "62           Pacific     Washington\n",
       "\n",
       "[63 rows x 2 columns]"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_df = pandas.read_csv('US States by Region - US Census Bureau - Sheet1.csv')\n",
    "cols = ['REGION', 'DIVISION']\n",
    "region_df.loc[:,cols] = region_df.loc[:,cols].ffill()\n",
    "region_df = region_df.drop('REGION', axis=1).rename(columns = {'DIVISION':'regional_division', 'STATE':'state'})\n",
    "region_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d8f79f-92a5-4a3c-9aa5-0387835551d1",
   "metadata": {},
   "source": [
    "The file containing the populations of the states and regions is downloaded from https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html. The file is then read in and formatted by dropping all the unnecessary columns and rows. a leading . before each state is removed as are the commas from the population as it will be used as a float data type during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a07c97-8175-42a3-b8a0-b9e98b39dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df = pandas.read_csv('state_population.csv')\n",
    "rel_columns = pop_df.drop(['Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3'], axis = 1)\n",
    "rel_rows = rel_columns.drop([0,1,2,61,62,63,64,65, 59]).reset_index(drop=True)\n",
    "rel_rows = rel_rows.rename(columns={'table with row headers in column A and column headers in rows 3 through 4. (leading dots indicate sub-parts)':'state', 'Unnamed: 4': 'population'})\n",
    "rel_rows = rel_rows.replace('^\\\\.', '', regex=True)\n",
    "rel_rows = rel_rows.replace(',','', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81fc93b-76ff-4da5-b0e7-66a683db79dd",
   "metadata": {},
   "source": [
    "The populations of each state are merged into the final dataset. An inner join is used over the state which acts as a foreign key. All values which are not states are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cbf662-d3d6-48bd-96c1-b606323a0572",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pandas.merge(left=data_to_analyze, right=rel_rows, left_on='state', right_on='state')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1936676-5cbd-4749-8453-da73035f930b",
   "metadata": {},
   "source": [
    "The regional divisions of each state are merged into the final dataset as well. An inner join is used over the state which acts as a foreign key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5fa4b-7291-467c-a176-0d7225714ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pandas.merge(left=merged_data, right=region_df, left_on='state', right_on='state')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d84ad-1690-4f7e-9854-e4a3a692aff3",
   "metadata": {},
   "source": [
    "In order to identify the 10 US states with the highest total articles per capita (in descending order), the subset of the relevant columns are first identified. It is then grouped by state and population and the total number of articles is identified. This is then divided by the state population to get the highest total articles per capita. The data frame is then sorted according to the requirements of the question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "b5ef4906-b841-484b-819a-7993bd4bcf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the relevant columns\n",
    "q1_data = final_data[['state','population']]\n",
    "#Group by state to find the number of articles per state\n",
    "q1_data_grouped = q1_data.groupby(['state', 'population']).size().reset_index()\n",
    "#Divide by state population which is converted to float to find the highest total articles per capita \n",
    "q1_data_grouped[0] = q1_data_grouped[0]/q1_data_grouped['population'].astype(float)\n",
    "#Sort the dataframe by the total articles per capita \n",
    "sorted_q1_data = q1_data_grouped.sort_values(by=0, ascending=False)\n",
    "sorted_q2_data = q1_data_grouped.sort_values(by=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7da45d-3a32-4797-9409-bdc781b46c6e",
   "metadata": {},
   "source": [
    "The 10 US states with the highest total articles per capita (in descending order) are identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "6c506405-b3f2-4bfc-aaf7-b772f37c4005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>population</th>\n",
       "      <th>Top 10 US states by coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>647064</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>779261</td>\n",
       "      <td>0.000457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maine</td>\n",
       "      <td>1385340</td>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>909824</td>\n",
       "      <td>0.000342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>3200517</td>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>733583</td>\n",
       "      <td>0.000203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>12972008</td>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>10034113</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>581381</td>\n",
       "      <td>0.000170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>1395231</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           state population  Top 10 US states by coverage\n",
       "0        Vermont     647064                      0.000508\n",
       "1   North Dakota     779261                      0.000457\n",
       "2          Maine    1385340                      0.000349\n",
       "3   South Dakota     909824                      0.000342\n",
       "4           Iowa    3200517                      0.000326\n",
       "5         Alaska     733583                      0.000203\n",
       "6   Pennsylvania   12972008                      0.000197\n",
       "7       Michigan   10034113                      0.000177\n",
       "8        Wyoming     581381                      0.000170\n",
       "9  New Hampshire    1395231                      0.000168"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_q1_data = sorted_q1_data.rename(columns = {0:\"Top 10 US states by coverage\"})\n",
    "sorted_q1_data.reset_index(drop=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8023f6be-1dc8-4561-b83e-771562fc0ebf",
   "metadata": {},
   "source": [
    "The 10 US states with the lowest total articles per capita (in ascending order) are identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "874bf02c-ad13-4e71-9963-a8294fb2b94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>population</th>\n",
       "      <th>Bottom 10 US states by coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>10698973</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>3177772</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>California</td>\n",
       "      <td>39029342</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>7359197</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>8683619</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Florida</td>\n",
       "      <td>22244823</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>4019800</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>2937150</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>6164660</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>5892539</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            state population  Bottom 10 US states by coverage\n",
       "0  North Carolina   10698973                         0.000005\n",
       "1          Nevada    3177772                         0.000006\n",
       "2      California   39029342                         0.000012\n",
       "3         Arizona    7359197                         0.000012\n",
       "4        Virginia    8683619                         0.000015\n",
       "5         Florida   22244823                         0.000019\n",
       "6        Oklahoma    4019800                         0.000019\n",
       "7          Kansas    2937150                         0.000021\n",
       "8        Maryland    6164660                         0.000025\n",
       "9       Wisconsin    5892539                         0.000032"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_q2_data = sorted_q2_data.rename(columns = {0:\"Bottom 10 US states by coverage\"})\n",
    "sorted_q2_data.reset_index(drop=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ae2cb-31f7-4886-89b8-e26d5e228c35",
   "metadata": {},
   "source": [
    "High quality articles are defined as the ones that have been classified as either FA or GA which stands for Featured Article or Good Article. To identify the number of high quality articles per capita, they are extracted into a new dataframe. They are then grouped by state and population and the total number of articles is identified which is then divided by the state population to get the highest total high quality articles per capita. The data frame is then sorted according to the requirements of the question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "9b96e4e9-c74f-4b8b-a0f4-524a7be7e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the relevant columns\n",
    "q3_data = final_data[['state','population','article_quality']]\n",
    "#Identify the FA and GA classified articles\n",
    "q3_data = q3_data.loc[q3_data['article_quality'].isin(['FA','GA'])]\n",
    "#Group by state to find the number of articles per state\n",
    "q3_data_grouped = q3_data.groupby(['state', 'population']).size().reset_index()\n",
    "#Divide by state population which is converted to float to find the highest total articles per capita \n",
    "q3_data_grouped[0] = q3_data_grouped[0]/q3_data_grouped['population'].astype(float)\n",
    "#Sort the dataframe by the number of high quality articles per capita \n",
    "sorted_q3_data = q3_data_grouped.sort_values(by=0, ascending=False)\n",
    "sorted_q4_data = q3_data_grouped.sort_values(by=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87eafdc-c932-4286-bddb-98c7f63450ca",
   "metadata": {},
   "source": [
    "The 10 US states with the highest high quality articles per capita (in descending order) are identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "5e7ababe-ce25-4dbe-83b1-08f106fc7f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>population</th>\n",
       "      <th>Top 10 US states by high quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>647064</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>581381</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>909824</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>1775156</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Montana</td>\n",
       "      <td>1122867</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>1395231</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>12972008</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>6177957</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>733583</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>9261699</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           state population  Top 10 US states by high quality\n",
       "0        Vermont     647064                          0.000070\n",
       "1        Wyoming     581381                          0.000067\n",
       "2   South Dakota     909824                          0.000062\n",
       "3  West Virginia    1775156                          0.000060\n",
       "4        Montana    1122867                          0.000049\n",
       "5  New Hampshire    1395231                          0.000045\n",
       "6   Pennsylvania   12972008                          0.000044\n",
       "7       Missouri    6177957                          0.000043\n",
       "8         Alaska     733583                          0.000042\n",
       "9     New Jersey    9261699                          0.000041"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_q3_data = sorted_q3_data.rename(columns = {0:\"Top 10 US states by high quality\"})\n",
    "sorted_q3_data[0:10].reset_index(drop=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf06201-3de9-4fb5-ab0c-bf35619e9bc3",
   "metadata": {},
   "source": [
    "The 10 US states with the lowest high quality articles per capita (in ascending order) are identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "b3a67351-516b-45ab-b3b6-c163978ad426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>population</th>\n",
       "      <th>Bottom 10 US states by high quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>10698973</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>8683619</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>3177772</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>7359197</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>39029342</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Florida</td>\n",
       "      <td>22244823</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>New York</td>\n",
       "      <td>19677151</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>6164660</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>2937150</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>4019800</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            state population  Bottom 10 US states by high quality\n",
       "0  North Carolina   10698973                             0.000002\n",
       "1        Virginia    8683619                             0.000002\n",
       "2          Nevada    3177772                             0.000003\n",
       "3         Arizona    7359197                             0.000003\n",
       "4      California   39029342                             0.000004\n",
       "5         Florida   22244823                             0.000005\n",
       "6        New York   19677151                             0.000006\n",
       "7        Maryland    6164660                             0.000007\n",
       "8          Kansas    2937150                             0.000007\n",
       "9        Oklahoma    4019800                             0.000008"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_q4_data = sorted_q4_data.rename(columns = {0:\"Bottom 10 US states by high quality\"})\n",
    "sorted_q4_data[0:10].reset_index(drop=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f73601-4873-4bb0-9a3f-df4149904a74",
   "metadata": {},
   "source": [
    "In order to identify total articles per capita for a regional division, we need to group by regional division and calculate the number of articles that are associated with that division. The other steps all remain the same as the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "8a38987e-19f8-4e85-bb56-3227154fec15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m2/gc3hzx697sqd2xsxwzcyyz0w0000gn/T/ipykernel_45419/850446491.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  q5_data['population'] = q5_data['population'].astype(float)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>Census divisions by total coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New England</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regional_division  Census divisions by total coverage\n",
       "0  West North Central                            0.000181\n",
       "1         New England                            0.000125\n",
       "2  East North Central                            0.000101\n",
       "3     Middle Atlantic                            0.000090\n",
       "4  East South Central                            0.000078\n",
       "5  West South Central                            0.000050\n",
       "6            Mountain                            0.000047\n",
       "7             Pacific                            0.000024\n",
       "8      South Atlantic                            0.000023"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identify the relevant columns\n",
    "q5_data = final_data[['regional_division', 'state', 'population']]\n",
    "#Convert the population column from string to float\n",
    "q5_data['population'] = q5_data['population'].astype(float)\n",
    "#Group by regional division and get the sum of populations as a new column\n",
    "q5_data_grouped_population = q5_data.groupby(['regional_division', 'state', 'population']).sum().reset_index()\n",
    "q5_data_grouped_population = q5_data_grouped_population.groupby(['regional_division'])['population'].sum().reset_index()\n",
    "#Group by regional division and get the number of articles for each regional division\n",
    "q5_data_grouped = q5_data.groupby(['regional_division']).size().reset_index()\n",
    "#Divide the two for per capita values\n",
    "q5_data_grouped[0] = q5_data_grouped[0]/q5_data_grouped_population['population']\n",
    "#sort in descending order\n",
    "sorted_q5_data = q5_data_grouped.sort_values(by=0, ascending=False)\n",
    "sorted_q5_data = sorted_q5_data.rename(columns = {0:\"Census divisions by total coverage\"})\n",
    "sorted_q5_data.reset_index(drop=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ed2f3-45ae-427f-a8e6-1635efb3ed52",
   "metadata": {},
   "source": [
    "In order to identify the number of total high quality articles per capita for a regional division, we need to group by regional division and calculate the number of articles that are associated with that division among the entries that are classified as either FA or GA. The other steps all remain the same as the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "2d3af203-a1cf-4b8c-a086-aff8b8998717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>Census divisions by high quality coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New England</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regional_division  Census divisions by high quality coverage\n",
       "0  West North Central                                   0.000032\n",
       "1     Middle Atlantic                                   0.000025\n",
       "2         New England                                   0.000020\n",
       "3  East South Central                                   0.000016\n",
       "4  East North Central                                   0.000015\n",
       "5  West South Central                                   0.000015\n",
       "6            Mountain                                   0.000013\n",
       "7             Pacific                                   0.000009\n",
       "8      South Atlantic                                   0.000008"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identify the relevant columns\n",
    "q6_data = final_data[['regional_division','state','article_quality','population']]\n",
    "#Extract the values classified as FA and GA\n",
    "q6_data = q6_data.loc[q6_data['article_quality'].isin(['FA','GA'])]\n",
    "##Convert the population column from string to float\n",
    "q6_data['population'] = q6_data['population'].astype(float)\n",
    "#Group by regional division and get the sum of populations as a new column\n",
    "q6_data_grouped_population = q6_data.groupby(['regional_division', 'state', 'population']).sum().reset_index()\n",
    "q6_data_grouped_population = q6_data_grouped_population.groupby(['regional_division'])['population'].sum().reset_index()\n",
    "#Group by regional division and get the number of articles for each regional division\n",
    "q6_data_grouped = q6_data.groupby(['regional_division']).size().reset_index()\n",
    "#Divide the two for per capita values\n",
    "q6_data_grouped[0] = q6_data_grouped[0]/q5_data_grouped_population['population']\n",
    "#sort in descending order\n",
    "sorted_q6_data = q6_data_grouped.sort_values(by=0, ascending=False)\n",
    "sorted_q6_data = sorted_q6_data.rename(columns = {0:\"Census divisions by high quality coverage\"})\n",
    "sorted_q6_data.reset_index(drop=True)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf12b3b-1138-4e6b-97ed-7dbd918cc05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
